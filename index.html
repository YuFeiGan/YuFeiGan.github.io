<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>GanYuFei (甘宇飞)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Blog about Research and Code">
<meta property="og:type" content="website">
<meta property="og:title" content="GanYuFei (甘宇飞)">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="GanYuFei (甘宇飞)">
<meta property="og:description" content="Blog about Research and Code">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GanYuFei (甘宇飞)">
<meta name="twitter:description" content="Blog about Research and Code">
  
    <link rel="alternative" href="/atom.xml" title="GanYuFei (甘宇飞)" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">GanYuFei (甘宇飞)</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">To improve is to change, to perfect is to change often.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-近期DL重要论文整理长期更新" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/26/近期DL重要论文整理长期更新/" class="article-date">
  <time datetime="2015-02-26T08:27:16.000Z" itemprop="datePublished">2015-02-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/26/近期DL重要论文整理长期更新/">近期DL重要论文整理长期更新</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="非线性单元：">非线性单元：</h1><p><code>Maxout</code> Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.</p>
<p><code>dropout</code> Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1): 1929–1958, 2014.</p>
<p><code>LReLU</code> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852, 2015.</p>
<p>目前非线性单元一般不破坏ReLU的结构而用非线性的运算方法接入网络层与层之间来产生非线性表达能力。</p>
<h1 id="增加模型深度：">增加模型深度：</h1><p><code>NIN</code> Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. 12 2013. URL <a href="http://arxiv.org/abs/1312.4400" target="_blank" rel="external">http://arxiv.org/abs/1312.4400</a>.</p>
<p><code>Inception/GoogLeNet</code> Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.</p>
<p>这里指的“深度”不光指层数，仅靠增加层数会带来训练困难。这里是指在有限层增加网络复杂程度。也可以说是非线性单元的一种变体。</p>
<h1 id="训练过程中的效率：">训练过程中的效率：</h1><p><code>LReLU</code> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852, 2015.</p>
<p><code>BatchNorm</code> Sergey Ioffe, Christian Szegedy,. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. </p>
<p>前者为参数提供了更加容易收敛初始值，后者防止训练过程中的梯度发散，两者都是解决同类问题，vanishing gradients（前）和exploding gradients（后）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/26/近期DL重要论文整理长期更新/" data-id="cibkqceg30003tg37ia00g5ra" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-《Deep-Learning》-Bengio-读书笔记3-CNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/" class="article-date">
  <time datetime="2015-02-20T07:43:28.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Book/">Book</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/">《Deep Learning》(Bengio)读书笔记3-CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>此书尚未出版，该笔记仅供学习参考，原文见<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external">http://www.iro.umontreal.ca/~bengioy/dlbook/</a></p>
<h1 id="Convolutional_Networks">Convolutional Networks</h1><h2 id="The_convolution_operation">The convolution operation</h2><p>Toeplitz matrix, Circulant matrix</p>
<h2 id="Motivation">Motivation</h2><ul>
<li>Sparse interactions: by making the kernel size smaller than the input.</li>
<li>Parameter sharing: using the same parameter for more than one function in model.</li>
<li><p>Equivariant representations: if the input changes, the output changes in the same way $f(g(x))=g(f(x))$, the convolution is not equivariant to some other transformations (change scale or rotation)</p>
<p>  Using convolution is an infinitely strong prior probability distribution over the parameters of a layer — that is the function of the layer should learn contains only local interactions and is equivariant to translation.</p>
</li>
</ul>
<p>The use of convolution constrains the class of functions that the layer can represent. If the necessary function does not have these properties, then using a convolutional layer will cause the model to have high training error.</p>
<p>Matrix multiplication cannot be applied to fixed-shape matrix, but convolution could.</p>
<h2 id="Pooling">Pooling</h2><p>CNN consists of three stages, 1.convolution, 2.nonlinear activation (sometimes called detector stage), 3.Pooling.</p>
<p>Pooling helps to make the representation become invariant to small translations of input.<br>    Invariance to local translation can be a very useful property if we care more about whether come feature is present than exactly where it is.</p>
<p>Invariance不关心特征位置只关心特征是否存在。</p>
<pre><code>Using pooling is <span class="operator">an</span> infinitely strong prior that <span class="operator">the</span> <span class="function"><span class="keyword">function</span> <span class="title">the</span> <span class="title">layer</span> <span class="title">learns</span> <span class="title">must</span> <span class="title">be</span> <span class="title">invariant</span> <span class="title">to</span> <span class="title">small</span> <span class="title">translations</span></span>
</code></pre><h2 id="Variants_of_the_basic_convolution_function">Variants of the basic convolution function</h2><p>differ form convolution operation in mathematical literature.</p>
<ul>
<li>it means many application of convolution</li>
<li>the input is a grid of vector-valued observation instead of real values.</li>
<li>be not guaranteed to be commutative.</li>
</ul>
<p>three special case of zero-padding</p>
<ul>
<li>valid convolution: kernel is contained within the image. $m-k+1\times m-k+1$</li>
<li>same convolution: keep the size of the output equal to the size of the input. $m\times m$</li>
<li>full convolution: be not guaranteed to be commutative. $m+k-1\times m+k-1$</li>
</ul>
<p>tiled convolution</p>
<h2 id="Data_types">Data types</h2><h2 id="Efficient_convolution_algorithms">Efficient convolution algorithms</h2><p>Parallel computation.</p>
<p>Using Fourier transform accelerate the speed of convolution</p>
<p>Devising faster ways of performing convolution or approximate convolution without harming the accuracy if the model is an active area of research.</p>
<p>In the commercial, deployment is more important than training.</p>
<h2 id="Deep_learning_history">Deep learning history</h2><p>Conv-Nets are first really successful deep net.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/" data-id="cibkqceg50004tg37ct4unimz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《Deep-Learning》-Bengio-读书笔记2-ML基本知识" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/" class="article-date">
  <time datetime="2015-02-20T07:43:28.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Book/">Book</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/">《Deep Learning》(Bengio)读书笔记2-ML基本知识</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>此书尚未出版，该笔记仅供学习参考，原文见<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external">http://www.iro.umontreal.ca/~bengioy/dlbook/</a></p>
<h1 id="Machine_Learning_Basics">Machine Learning Basics</h1><p>本部分介绍机器学习的基本概念。</p>
<h2 id="what_is_learning?">what is learning?</h2><p>Definition of learning: “A computer program is said to learn from experience $E$ with respect to some class of task $T$ and proformance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$”</p>
<h3 id="The_task,_$T$">The task, $T$</h3><p>Classification, Classification with missing input, Regression, Transcription, Density estimation, Anomaly detection, Synthesis and sampling, Imputation of missing values.</p>
<h3 id="The_proformance_measure,_$P$">The proformance measure, $P$</h3><p>accuracy of the model or the probability of the model to some examples.</p>
<p>Using test data to evaluate the proformance.</p>
<p>we always use loss to represent the cost associated with a particular event (such as a classification).</p>
<p>the objective of learning is then to minimize the loss</p>
<p>choice error as loss for classification tasks</p>
<p>the choice of performance measure depend on the application.</p>
<p>In order for a performance measure to be optimized directly. we almost always require a smooth signal — a gradient</p>
<p>in place of the natural loss functions, we often use surrogate performance measure (also called surrogate loss functions) that are amenable for direct use as the objective function optimized with respect to the model parameters. </p>
<h3 id="The_experience,_$E$">The experience, $E$</h3><p>example, label.</p>
<p>design matrix — in some case, feature have different size</p>
<h2 id="Generalization,_Capacity,_Overfitting_and_Underfitting">Generalization, Capacity, Overfitting and Underfitting</h2><h3 id="Generalization">Generalization</h3><p>generalization error.</p>
<p>Formally, generalization performance is typically defined as the expected value of the chosen performance measure, taken over the probability distribution of interest.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/" data-id="cibkqceg80009tg379obtfusz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《Deep-Learning》-Bengio-读书笔记1-DL简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/" class="article-date">
  <time datetime="2015-02-20T07:43:28.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Book/">Book</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/">《Deep Learning》(Bengio)读书笔记1-DL简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>此书尚未出版，该笔记仅供学习参考，原文见<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external">http://www.iro.umontreal.ca/~bengioy/dlbook/</a></p>
<p>[P2]早期的AI是以IBM为的深蓝代表一类的简单的AI。</p>
<p>人的知识与经验是主观和直觉的，因此难以用一种正式的方法表达出来。电脑却恰恰需要各种形式的知识去做决策。</p>
<p>几个AI项目曾经探索用正式语言的硬编码去描述知识。计算机可以运用逻辑推断法则通过这些正式语言推断陈述。但这些AI项目的均未成功，[P3]其中最有名的就是Cyc（人们用十分复杂的语言来准确的描述世界）。</p>
<p>hard-code所面临的困难说明AI系统需要有能力从原始数据中去获得他们自己的知识。这种能力就称为ML，ML的引入让计算机应付真实世界的问题并做出近乎于主观的决策。</p>
<p>LR是一种简单的ML算法，曾被来预测是否推荐做剖腹产另一种叫做朴素贝叶斯的算法可以分辨垃圾邮件。</p>
<p>我们称执行学习过程的代理为Learning Machine或者Learner，它将训练数据作为输入，产生函数。</p>
<p>这些简单的ML算法的性能，非常依赖于所给数据的表达。Represent中的每一部分信息就是feature。LR学习每个feature所对应的不同的产生效果。然而LR无法学习出哪些特征有用，哪些特征无用，比如说若输入是像素点，单个像素点对后面的结果无关轻重。</p>
<p>这个对表达的依赖是伴随计算机科学的普遍现象，表达的选择对ML算法的选择有极大的影响。</p>
<p>[P4]许多AI问题通过设计恰当的特征的到解决，然后把这些特征给一个简单的ML算法。[还有些问题不能通过特征解决？]</p>
<p>然而对于有些问题，什么特征应该别提取显得非常困难。比如车的轮子。</p>
<p>一种解决方案是，让ML不光学习Representation到output的映射，还学习表达本身。这种方法称为Representation Learning，RL的效果往往比hand-designed representation要好。它们同样允许AI系统在最小的人工干预下快速适应新的问题。</p>
<p>[P5]当设计特征的时候我们的目标往往是分离解释观测数据的变量因素。这里的factor仅是影响源，通常不包含mutiplication。这样的factor通常不是通过直接观察得到的，而是存在于人的思想中用来解释或推断观测数据的原因。这些factor可以被理解为帮助我们理解数据中大量变量的概念或抽象。</p>
<p>真实世界中的AI应用的一个主要困难是许多factors影响每一个我们所观测的数据。许多应用希望我们从大量的factor中解脱出来并且摈弃我们不在乎的factor。</p>
<p>当然，从原始数据中提取high-level抽象特征十分困难（不少需要人类级别的理解）。RL这一看无法解决这个问题。</p>
<p>DL解决这个RL中的核心问题通过引入更简单的表达。DL允许电脑建立由简单概念产生的复杂概念。</p>
<p>DL的另一个远景是他允许计算机学习一个多部的计算机程序（没看明白？？）</p>
<p>Depth在这里不是数学严格定义上的意思。在DL中它没有正式的定义。有些方法大概表达学习的次数多，另一些方法中指计算的概念深。因为简单概念可以理解为复杂概念。</p>
<p>[P7]简而言之，DL是一种AI方法，ML的一种类型。DL is a particular kind of ML that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts with each concept defined in relation to simpler concepts.</p>
<p>[P9]DL在过去5年中，对工业界和学术界产生了极大的影响，DL涉及到Learning multiple levels of representation。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/" data-id="cibkqcega000ctg37j41srdr2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Note" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/23/Note/" class="article-date">
  <time datetime="2014-11-23T06:33:16.000Z" itemprop="datePublished">2014-11-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Note/">Note</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/23/Note/">Note</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文根据半年来对相关资料以及实验操作的实际体会总结而成。限于本人知识经验局限性，难免有理解错误之处。然而还是把一些不太成熟的想法写了下来。<br>最近对整个CNN的形成有了一个模糊的框架，隐约感觉以下3篇还有下面涉及到或没有涉及但我看过的文章都有深层的联系。可能由于知识有限，表达可能不尽人意，但还是想试图解释他们之间的关系。</p>
<h3 id="概括">概括</h3><h6 id="PCA文章总结">PCA文章总结</h6><p>PCANet的文章揭示了深层网络结构的优越性，证实cascaded feature learning or extraction architectures的优越性。而且由于采用PCA的方法，不需要用迭代的方法寻优。可以从数据得到filter参数。在PCANet结构上也可以看出：如果要构建自己的网络需要理解每层的特点，根据实际情况组合网络，在别的网络中效果好的结构未必自己的网络中就好。文章称PCANet可以做baseline，说明调参技巧少，对trick等依赖少，训练效果变化小。</p>
<h6 id="Feature_coding_for_classification文章总结">Feature coding for classification文章总结</h6><p>总结了大量降维方法。提到5个大块方法其中一个大块是manifold，流形逼近作为一种非参数模型方法，将会是一个方向。概率模型都对数据分布有较强的的假设，如果实际数据不服从假设的分布，会对效果造成影响。因此选用概率模型时要对数据分布有准确的估计。本文最后同样指出了设计特征时要考虑结合feature coding and pooling。<br>improved Fisher kernel具有目前已知最好的性能，用GMM描述概率分布</p>
<h6 id="图嵌入">图嵌入</h6><p>这篇文章有很强的理论性。用一个框架总结PCA、LDA(linear), ISOMAP、LLE、Laplacian Eigenmap(no-linear)等降维方法。<br>特征编码中提到：高阶特征流形逼近的方法作为一种非参数模型，将更好地改善特征编码的概率密度分布估计。<br>本文给出的思路有助于改善对概率密度分布估计的困难。</p>
<h3 id="DL相关总结">DL相关总结</h3><p>DL的核心思想是deep，深度的网络结构是提升学习效率的重要原因。以往的神经网络属于浅度学习层数少，由于深层训练在训练方法和运算能力上的突破兴起了深度学习浪潮。<br>（变换+降维）+（变换+降维）+………+（变换+降维）+（变换+降维）</p>
<p>（非线性层ReLU目前被认为具有最好的非线性表达效果，因此在网络中比较常用。max(0,x), ln(1+ex)）原因未知？</p>
<h6 id="DL成功的关键：">DL成功的关键：</h6><p>一方面现在的深层网络可以在高层学习到更高级的特征（语意特征），反卷积网络可视化技术证实了这一点，另一方面而稀疏表达和自编码等技术则能帮助深层网络在底层学习到比传统人工构造的特征更具有表达性的特征。</p>
<h6 id="CNN复兴的原因：（Visualizing_and_Understanding_Convolutional_Networks）Matthew_D-_Zeiler">CNN复兴的原因：（Visualizing and Understanding Convolutional Networks）Matthew D. Zeiler</h6><ol>
<li>拥有数以百万带标签训练集出现</li>
<li>基于GPU训练算法的出现，使得训练复杂卷积网模型不再是奢望</li>
<li>更好的模型调优策略（dropout）</li>
</ol>
<h6 id="Baidu总结的DL成功原因">Baidu总结的DL成功原因</h6><ol>
<li>Big Data</li>
<li>high performance computing(取得了想象不到的超越实验室的效果）</li>
</ol>
<p>mnist数据集本身稀疏，没有用pooling效果也好。<br>训练与测试不同（dropout、multi-scale、multi-view）大多是为了解决数据量问题。多的数据量，更多的数据。<br>Visualizing and Understanding Convolutional Networks中ImageNet 2012上在单块GTX580 GPU， 迭代70次用了12天。内存开销与时间开销都很大。</p>
<h6 id="与PCANet的联系">与PCANet的联系</h6><p>CNN的训练过程是迭代寻优，而某些流形学习方法可以得到解析解（PCA也有在线学习算法），不需要迭代，这样训练速度将大大提升。<br>CNN训练极大程度依赖调参和trick（因为迭代，容易收敛到局部最优），得到好效果不容易。但PCA参数少，所以他说可以做baseline。<br>另一个角度来看，可以认为没有解析解的CNN会有更强大的非线性表达能力，人工干预较强的构造性网络PCANet效果会受影响。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/11/23/Note/" data-id="cibkqcef60000tg37mumt6rpr" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Torch7-A-Matlab-like-Environment-for-Machine-Learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/23/Torch7-A-Matlab-like-Environment-for-Machine-Learning/" class="article-date">
  <time datetime="2014-11-23T06:26:16.000Z" itemprop="datePublished">2014-11-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Note/">Note</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/23/Torch7-A-Matlab-like-Environment-for-Machine-Learning/">Torch7 A Matlab-like Environment for Machine Learning</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Torch7:_A_Matlab-like_Environment_for_Machine_Learning">Torch7: A Matlab-like Environment for Machine Learning</h3><p>Why not Python? It is hard to talk about a language without starting a flame war. While Lua is well known in the gaming programmer community (because of its speed advantage and great em- bedding capabilities), Python is more popular in a more general public. With no doubt, Python ships with more libraries. However, with no doubt5, “Integrating Lua with C is so easy a child could do it. Lua was designed for this also, from the beginning, and it shows6. This means that with a few hours’ work, any C or C++ library can become a Lua library.”. Another key advantage of Lua is its embedding capabilities: once code has been prototyped, it can be turned into a final system/product with very little extra work. Extra performance can be obtained using LuaJIT, yielding C-like performance for most of the pure Lua code. Lua being written in pure ANSI C, it can be easily compiled for arbitrary targets (cell-phones, embedded CPUs in FPGAs, DSP processors, …). Adding Lua’ speed advantage, the choice was a “no brainer”.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/11/23/Torch7-A-Matlab-like-Environment-for-Machine-Learning/" data-id="cibkqcegc000ftg37xdx0afea" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Torch/">Torch</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Book/">Book</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Note/">Note</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Torch/">Torch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Torch/" style="font-size: 10px;">Torch</a> <a href="/tags/《Deep-Learning》/" style="font-size: 20px;">《Deep Learning》</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/02/26/近期DL重要论文整理长期更新/">近期DL重要论文整理长期更新</a>
          </li>
        
          <li>
            <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/">《Deep Learning》(Bengio)读书笔记3-CNN</a>
          </li>
        
          <li>
            <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/">《Deep Learning》(Bengio)读书笔记2-ML基本知识</a>
          </li>
        
          <li>
            <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/">《Deep Learning》(Bengio)读书笔记1-DL简介</a>
          </li>
        
          <li>
            <a href="/2014/11/23/Note/">Note</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YuFeiGan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>