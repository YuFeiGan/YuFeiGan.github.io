<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>GanYuFei (甘宇飞)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Blog about Research and Code">
<meta property="og:type" content="website">
<meta property="og:title" content="GanYuFei (甘宇飞)">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="GanYuFei (甘宇飞)">
<meta property="og:description" content="Blog about Research and Code">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GanYuFei (甘宇飞)">
<meta name="twitter:description" content="Blog about Research and Code">
  
    <link rel="alternative" href="/atom.xml" title="GanYuFei (甘宇飞)" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">GanYuFei (甘宇飞)</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">To improve is to change, to perfect is to change often.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-近期DL重要论文整理长期更新" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/26/近期DL重要论文整理长期更新/" class="article-date">
  <time datetime="2015-02-26T08:27:16.000Z" itemprop="datePublished">2015-02-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/26/近期DL重要论文整理长期更新/">近期DL重要论文整理长期更新</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="非线性单元：">非线性单元：</h1><p><code>Maxout</code> Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.</p>
<p><code>dropout</code> Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1): 1929–1958, 2014.</p>
<p><code>LReLU</code> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852, 2015.</p>
<p>目前非线性单元一般不破坏ReLU的结构而用非线性的运算方法接入网络层与层之间来产生非线性表达能力。</p>
<h1 id="增加模型深度：">增加模型深度：</h1><p><code>NIN</code> Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. 12 2013. URL <a href="http://arxiv.org/abs/1312.4400" target="_blank" rel="external">http://arxiv.org/abs/1312.4400</a>.</p>
<p><code>Inception/GoogLeNet</code> Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.</p>
<p>这里指的“深度”不光指层数，仅靠增加层数会带来训练困难。这里是指在有限层增加网络复杂程度。也可以说是非线性单元的一种变体。</p>
<h1 id="训练过程中的效率：">训练过程中的效率：</h1><p><code>LReLU</code> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852, 2015.</p>
<p><code>BatchNorm</code> Sergey Ioffe, Christian Szegedy,. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. </p>
<p>前者为参数提供了更加容易收敛初始值，后者防止训练过程中的梯度发散，两者都是解决同类问题，vanishing gradients（前）和exploding gradients（后）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/26/近期DL重要论文整理长期更新/" data-id="cibkqhjv00005x937wqmopz46" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-《Deep-Learning》-Bengio-读书笔记3-CNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/" class="article-date">
  <time datetime="2015-02-20T07:43:28.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Book/">Book</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/">《Deep Learning》(Bengio)读书笔记3-CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>此书尚未出版，该笔记仅供学习参考，原文见<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external">http://www.iro.umontreal.ca/~bengioy/dlbook/</a></p>
<h1 id="Convolutional_Networks">Convolutional Networks</h1><h2 id="The_convolution_operation">The convolution operation</h2><p>Toeplitz matrix, Circulant matrix</p>
<h2 id="Motivation">Motivation</h2><ul>
<li>Sparse interactions: by making the kernel size smaller than the input.</li>
<li>Parameter sharing: using the same parameter for more than one function in model.</li>
<li><p>Equivariant representations: if the input changes, the output changes in the same way $f(g(x))=g(f(x))$, the convolution is not equivariant to some other transformations (change scale or rotation)</p>
<p>  Using convolution is an infinitely strong prior probability distribution over the parameters of a layer — that is the function of the layer should learn contains only local interactions and is equivariant to translation.</p>
</li>
</ul>
<p>The use of convolution constrains the class of functions that the layer can represent. If the necessary function does not have these properties, then using a convolutional layer will cause the model to have high training error.</p>
<p>Matrix multiplication cannot be applied to fixed-shape matrix, but convolution could.</p>
<h2 id="Pooling">Pooling</h2><p>CNN consists of three stages, 1.convolution, 2.nonlinear activation (sometimes called detector stage), 3.Pooling.</p>
<p>Pooling helps to make the representation become invariant to small translations of input.<br>    Invariance to local translation can be a very useful property if we care more about whether come feature is present than exactly where it is.</p>
<p>Invariance不关心特征位置只关心特征是否存在。</p>
<pre><code>Using pooling is <span class="operator">an</span> infinitely strong prior that <span class="operator">the</span> <span class="function"><span class="keyword">function</span> <span class="title">the</span> <span class="title">layer</span> <span class="title">learns</span> <span class="title">must</span> <span class="title">be</span> <span class="title">invariant</span> <span class="title">to</span> <span class="title">small</span> <span class="title">translations</span></span>
</code></pre><h2 id="Variants_of_the_basic_convolution_function">Variants of the basic convolution function</h2><p>differ form convolution operation in mathematical literature.</p>
<ul>
<li>it means many application of convolution</li>
<li>the input is a grid of vector-valued observation instead of real values.</li>
<li>be not guaranteed to be commutative.</li>
</ul>
<p>three special case of zero-padding</p>
<ul>
<li>valid convolution: kernel is contained within the image. $m-k+1\times m-k+1$</li>
<li>same convolution: keep the size of the output equal to the size of the input. $m\times m$</li>
<li>full convolution: be not guaranteed to be commutative. $m+k-1\times m+k-1$</li>
</ul>
<p>tiled convolution</p>
<h2 id="Data_types">Data types</h2><h2 id="Efficient_convolution_algorithms">Efficient convolution algorithms</h2><p>Parallel computation.</p>
<p>Using Fourier transform accelerate the speed of convolution</p>
<p>Devising faster ways of performing convolution or approximate convolution without harming the accuracy if the model is an active area of research.</p>
<p>In the commercial, deployment is more important than training.</p>
<h2 id="Deep_learning_history">Deep learning history</h2><p>Conv-Nets are first really successful deep net.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/" data-id="cibkqhjv10006x937qai4hmig" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《Deep-Learning》-Bengio-读书笔记2-ML基本知识" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/" class="article-date">
  <time datetime="2015-02-20T07:43:28.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Book/">Book</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/">《Deep Learning》(Bengio)读书笔记2-ML基本知识</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>此书尚未出版，该笔记仅供学习参考，原文见<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external">http://www.iro.umontreal.ca/~bengioy/dlbook/</a></p>
<h1 id="Machine_Learning_Basics">Machine Learning Basics</h1><p>本部分介绍机器学习的基本概念。</p>
<h2 id="what_is_learning?">what is learning?</h2><p>Definition of learning: “A computer program is said to learn from experience $E$ with respect to some class of task $T$ and proformance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$”</p>
<h3 id="The_task,_$T$">The task, $T$</h3><p>Classification, Classification with missing input, Regression, Transcription, Density estimation, Anomaly detection, Synthesis and sampling, Imputation of missing values.</p>
<h3 id="The_proformance_measure,_$P$">The proformance measure, $P$</h3><p>accuracy of the model or the probability of the model to some examples.</p>
<p>Using test data to evaluate the proformance.</p>
<p>we always use loss to represent the cost associated with a particular event (such as a classification).</p>
<p>the objective of learning is then to minimize the loss</p>
<p>choice error as loss for classification tasks</p>
<p>the choice of performance measure depend on the application.</p>
<p>In order for a performance measure to be optimized directly. we almost always require a smooth signal — a gradient</p>
<p>in place of the natural loss functions, we often use surrogate performance measure (also called surrogate loss functions) that are amenable for direct use as the objective function optimized with respect to the model parameters. </p>
<h3 id="The_experience,_$E$">The experience, $E$</h3><p>example, label.</p>
<p>design matrix — in some case, feature have different size</p>
<h2 id="Generalization,_Capacity,_Overfitting_and_Underfitting">Generalization, Capacity, Overfitting and Underfitting</h2><h3 id="Generalization">Generalization</h3><p>generalization error.</p>
<p>Formally, generalization performance is typically defined as the expected value of the chosen performance measure, taken over the probability distribution of interest.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/" data-id="cibkqhjv4000bx937nx63elgt" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-《Deep-Learning》-Bengio-读书笔记1-DL简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/" class="article-date">
  <time datetime="2015-02-20T07:43:28.000Z" itemprop="datePublished">2015-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Book/">Book</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/">《Deep Learning》(Bengio)读书笔记1-DL简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>此书尚未出版，该笔记仅供学习参考，原文见<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external">http://www.iro.umontreal.ca/~bengioy/dlbook/</a></p>
<p>[P2]早期的AI是以IBM为的深蓝代表一类的简单的AI。</p>
<p>人的知识与经验是主观和直觉的，因此难以用一种正式的方法表达出来。电脑却恰恰需要各种形式的知识去做决策。</p>
<p>几个AI项目曾经探索用正式语言的硬编码去描述知识。计算机可以运用逻辑推断法则通过这些正式语言推断陈述。但这些AI项目的均未成功，[P3]其中最有名的就是Cyc（人们用十分复杂的语言来准确的描述世界）。</p>
<p>hard-code所面临的困难说明AI系统需要有能力从原始数据中去获得他们自己的知识。这种能力就称为ML，ML的引入让计算机应付真实世界的问题并做出近乎于主观的决策。</p>
<p>LR是一种简单的ML算法，曾被来预测是否推荐做剖腹产另一种叫做朴素贝叶斯的算法可以分辨垃圾邮件。</p>
<p>我们称执行学习过程的代理为Learning Machine或者Learner，它将训练数据作为输入，产生函数。</p>
<p>这些简单的ML算法的性能，非常依赖于所给数据的表达。Represent中的每一部分信息就是feature。LR学习每个feature所对应的不同的产生效果。然而LR无法学习出哪些特征有用，哪些特征无用，比如说若输入是像素点，单个像素点对后面的结果无关轻重。</p>
<p>这个对表达的依赖是伴随计算机科学的普遍现象，表达的选择对ML算法的选择有极大的影响。</p>
<p>[P4]许多AI问题通过设计恰当的特征的到解决，然后把这些特征给一个简单的ML算法。[还有些问题不能通过特征解决？]</p>
<p>然而对于有些问题，什么特征应该别提取显得非常困难。比如车的轮子。</p>
<p>一种解决方案是，让ML不光学习Representation到output的映射，还学习表达本身。这种方法称为Representation Learning，RL的效果往往比hand-designed representation要好。它们同样允许AI系统在最小的人工干预下快速适应新的问题。</p>
<p>[P5]当设计特征的时候我们的目标往往是分离解释观测数据的变量因素。这里的factor仅是影响源，通常不包含mutiplication。这样的factor通常不是通过直接观察得到的，而是存在于人的思想中用来解释或推断观测数据的原因。这些factor可以被理解为帮助我们理解数据中大量变量的概念或抽象。</p>
<p>真实世界中的AI应用的一个主要困难是许多factors影响每一个我们所观测的数据。许多应用希望我们从大量的factor中解脱出来并且摈弃我们不在乎的factor。</p>
<p>当然，从原始数据中提取high-level抽象特征十分困难（不少需要人类级别的理解）。RL这一看无法解决这个问题。</p>
<p>DL解决这个RL中的核心问题通过引入更简单的表达。DL允许电脑建立由简单概念产生的复杂概念。</p>
<p>DL的另一个远景是他允许计算机学习一个多部的计算机程序（没看明白？？）</p>
<p>Depth在这里不是数学严格定义上的意思。在DL中它没有正式的定义。有些方法大概表达学习的次数多，另一些方法中指计算的概念深。因为简单概念可以理解为复杂概念。</p>
<p>[P7]简而言之，DL是一种AI方法，ML的一种类型。DL is a particular kind of ML that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts with each concept defined in relation to simpler concepts.</p>
<p>[P9]DL在过去5年中，对工业界和学术界产生了极大的影响，DL涉及到Learning multiple levels of representation。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/" data-id="cibkqhjv6000ex9375f9hm3eg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Caffe学习笔记5-BLAS与boost-thread加速" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/02/Caffe学习笔记5-BLAS与boost-thread加速/" class="article-date">
  <time datetime="2015-01-02T08:53:57.000Z" itemprop="datePublished">2015-01-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LearningCaffe/">LearningCaffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/02/Caffe学习笔记5-BLAS与boost-thread加速/">Caffe学习笔记5-BLAS与boost::thread加速</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Caffe中运用了大量的优化方法，最近在优化自己代码时候恰好运用了其中的BLAS和Boost::thread。使用过程中遇到了不少问题，这里把我遇到的问题和解决方法整理一下。</p>
<h1 id="BLAS是什么？">BLAS是什么？</h1><p>BLAS (Basic Linear Algebra Subprograms)基础线性代数子程序库，是一个应用程序接口（API）标准，说的简单点就是向量、矩阵之间乘加这些运算。BLAS虽然本身就有实现但效率不高，因此有大量的开源或商业项目对BLAS进行优化比如OpenBLAS（开源），Intel MKL（收费），ATLAS（开源）。我用的是OpenBLAS这个库。</p>
<h1 id="OpenBLAS">OpenBLAS</h1><p>OpenBLAS是C语言实现的，这个库安装比较简单，没有什么问题，唯一的一个问题是使用方法。前面介绍BLAS提供了接口，文档在<a href="http://www.netlib.org/blas/blasqr.pdf" target="_blank" rel="external">这里</a>。<br>这个文档中 Level 1 是vector与vector的操作，Level 2 是vector与matrix的操作，Level 3是matrix与matrix的操作。</p>
<p>每个函数的开头有一个x表示精度比如替换成s表示float类型，d表示double类型。</p>
<p>其实虽然函数很多但其实使用方法大同小异，BLAS之所以分的这么细（区分到对称矩阵，三角矩阵）是为了方便针对不同的情况做优化。所以其实搞清楚最关键的矩阵与矩阵的运算就已经理解了一大半。</p>
<p>以dgemm为例，全称为double-precision generic matrix-matrix muliplication，就是矩阵相乘，在OpenBLAS中的声明是:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cblas_dgemm(<span class="keyword">const</span> <span class="keyword">enum</span> CBLAS_ORDER Order,</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">enum</span> CBLAS_TRANSPOSE TransA,</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">enum</span> CBLAS_TRANSPOSE TransB,</span><br><span class="line">            <span class="keyword">const</span> blasint M,</span><br><span class="line">            <span class="keyword">const</span> blasint N,</span><br><span class="line">            <span class="keyword">const</span> blasint K,</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">double</span> alpha,</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">double</span> *A,</span><br><span class="line">            <span class="keyword">const</span> blasint lda,</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">double</span> *B,</span><br><span class="line">            <span class="keyword">const</span> blasint ldb,</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">double</span> beta,</span><br><span class="line">            <span class="keyword">double</span> *C,</span><br><span class="line">            <span class="keyword">const</span> blasint ldc)</span><br></pre></td></tr></table></figure>
<p>对应的公式如下：<br>$$C\leftarrow\alpha op(A)op(B)+\beta C, op(X)=X, X^T, X^H, C-m\times n$$</p>
<p>参数的说明如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">enum</span> CBLAS_ORDER Order,               \\指定矩阵的存储方式如RowMajor</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">enum</span> CBLAS_TRANSPOSE TransA,          \\A运算后是否转置</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">enum</span> CBLAS_TRANSPOSE TransB,          \\B运算后是否转置</span><br><span class="line"><span class="keyword">const</span> blasint M,                            \\A的行数</span><br><span class="line"><span class="keyword">const</span> blasint N,                            \\B的列数</span><br><span class="line"><span class="keyword">const</span> blasint K,                            \\A的列数</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> alpha,                         \\公式中的alpha</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> *A,                            \\A</span><br><span class="line"><span class="keyword">const</span> blasint lda,                          \\A一行的存储间隔</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> *B,                            \\B</span><br><span class="line"><span class="keyword">const</span> blasint ldb,                          \\B一行的存储间隔</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> beta,                          \\公式中的beta</span><br><span class="line"><span class="keyword">double</span> *C,                                  \\C</span><br><span class="line"><span class="keyword">const</span> blasint ldc                           \\C一行的存储间隔</span><br></pre></td></tr></table></figure></p>
<p>因为这里的A、B、C矩阵都是以一维数组的形式存储所以需要告诉函数他们一行的存储间隔就是lda、ldb、ldc它们。</p>
<p><a href="https://github.com/xianyi/OpenBLAS/wiki/User-Manual" target="_blank" rel="external">OpenBLAS wiki</a>上面的demo</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">include</span> &lt;cblas.h&gt;</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> &lt;stdio.h&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span><br><span class="line"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">double</span> A[<span class="number">6</span>] = &#123;<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">1.0</span>,-<span class="number">3.0</span>,<span class="number">4.0</span>,-<span class="number">1.0</span>&#125;;         </span><br><span class="line">  <span class="keyword">double</span> B[<span class="number">6</span>] = &#123;<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">1.0</span>,-<span class="number">3.0</span>,<span class="number">4.0</span>,-<span class="number">1.0</span>&#125;;  </span><br><span class="line">  <span class="keyword">double</span> C[<span class="number">9</span>] = &#123;.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>,.<span class="number">5</span>&#125;; </span><br><span class="line">  cblas_dgemm(CblasColMajor, CblasNoTrans, CblasTrans,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,A, <span class="number">3</span>, B, <span class="number">3</span>,<span class="number">2</span>,C,<span class="number">3</span>);</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">0</span>; i&lt;<span class="number">9</span>; i++)</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%lf "</span>, C[i]);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Boost是什么？">Boost是什么？</h1><p>用<a href="http://baike.baidu.com/link?url=W5XnZZwZBBknw49TQlptNQ33fvYF2EfU3wZTN9NSvUVxcgtwMInwJLPaCiVGAjZpfubi9-vRgrbY7qejPpyX7z1wD8LY7U3gczavikzEUCu" target="_blank" rel="external">百度百科</a>的话说，Boost库是一个经过千锤百炼、可移植、提供源代码的C++库,作为标准库的后备,是C++标准化进程的发动机之一。</p>
<p>实际感受就是一个相当强大的C++拓展库，很多C++标准库里面没有的功能得以实现。最近就用到了ublas，thread，date_time这三个模块。这里做一些简要的介绍。</p>
<h2 id="ublas">ublas</h2><p>调用位置boost::numeric::ublas，是boost的BLAS实现，虽然速度一般，但用起来非常方便可以直接用 + - 运算符号操作，还可以直接用 &lt;&lt; &gt;&gt; 等标准输入输出流。</p>
<h2 id="date_time">date_time</h2><p>在我计算多线程运行时间的时候发现标准C++提供的std::clock_t对于多CPU跑线程的情况会把几个CPU的时间加在一起，所以采用了 boost::posix_time::ptime 这种类型计数。解决了计时不准确的问题。</p>
<h2 id="thread">thread</h2><p>Boost提供的thread虽然功能据说不是非常强大，但是由于使用C++的思想重新设计，使用起来相对比较方便。网上文档非常多，比如：</p>
<ol>
<li><a href="http://www.cnblogs.com/chengmin/archive/2011/12/29/2306416.html" target="_blank" rel="external">C++ Boost Thread 编程指南</a></li>
<li><a href="http://blog.csdn.net/zhuxiaoyang2000/article/details/6588031" target="_blank" rel="external">Boost::Thread使用示例</a><br>在实际运用中还是发现不少问题比如<h3 id="线程的并行化">线程的并行化</h3>用一般的方法并不能真正的并行程序，比如<a href="http://stackoverflow.com/questions/6215511/how-to-run-multiple-threads-created-by-loop-simultaneous-using-boost-thread/6215646#6215646" target="_blank" rel="external">]这里的代码</a>：<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;boost::thread *&gt; z;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">z.push_back(<span class="keyword">new</span> boost::thread(workerFunc));</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)&#123;</span><br><span class="line">    z[i]-&gt;join();</span><br><span class="line">    <span class="keyword">delete</span> z[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>即使是这样在某些情况下我的线程还是不能并行运行，只是同时开始创建而已。<br>解决方案是使用<a href="http://stackoverflow.com/questions/3344028/how-to-make-boostthread-group-execute-a-fixed-number-of-parallel-threads" target="_blank" rel="external">boost::thread_group</a><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">boost::thread_group group;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">15</span>; ++i)</span><br><span class="line">    group.<span class="keyword">create_t</span>hread(aFunctionToExecute);</span><br><span class="line">group.join_all();</span><br></pre></td></tr></table></figure></p>
<p>当执行join_all()的时候才是真正的并行了程序。</p>
<h3 id="成员函数没有实例但又要传参的方法">成员函数没有实例但又要传参的方法</h3><p>编译时候错误：error: reference to non-static member function must be called; did you mean to call it with no arguments?<br>查了Google发现是因为我定义的类中的成员函数用group.create_thread()中调用了没有实例化的成员函数解决方法是<a href="http://stackoverflow.com/questions/20250860/can-you-initialize-a-thread-in-a-class-which-references-a-function-inside-that-c" target="_blank" rel="external">使用std::bind</a><br>我直接用的是boost::bind<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">group.<span class="keyword">create_t</span>hread(boost::bind(&amp;myfunction, <span class="keyword">this</span>));</span><br></pre></td></tr></table></figure></p>
<h3 id="线程同步互斥锁boost::mutex">线程同步互斥锁boost::mutex</h3><p>接触过多线程的人一定不会陌生，当我们用操作操作统一数据的读写时或是数据输出输出时必须要用到互斥量。一开始用boost::mutex::scoped_lock给boost::mutex上锁时只知道用<a href="http://blog.csdn.net/fengge8ylf/article/details/6683527" target="_blank" rel="external">boost::condition</a>控制线程的状态，后来发现代码越写越复杂，后来发现只需要改变boost::mutex作用域就可以自动给boost::mutex lock/unlock。比如如果每次只让一个线程输出信息到屏幕：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">boost::mutex io_mutex;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line">    &#123;</span><br><span class="line">        boost::mutex::<span class="function">scoped_lock <span class="title">lock</span><span class="params">(io_mutex)</span></span>;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"something output!"</span> &lt;&lt; <span class="built_in">std</span>::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// something to do!</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>用这种方法多个函数在对统一个数据操作的时候就不会有冲突了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/01/02/Caffe学习笔记5-BLAS与boost-thread加速/" data-id="cibkqhjvl0013x937xl42txne" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BLAS/">BLAS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Boost/">Boost</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Thread/">Thread</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Caffe学习笔记4-caffe安装需要注意的libraries" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/12/14/Caffe学习笔记4-caffe安装需要注意的libraries/" class="article-date">
  <time datetime="2014-12-14T02:58:09.000Z" itemprop="datePublished">2014-12-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LearningCaffe/">LearningCaffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/12/14/Caffe学习笔记4-caffe安装需要注意的libraries/">Caffe学习笔记4-caffe安装需要注意的libraries</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>当调用caffe训练数据的时候实际上调用的文件是在caffe.cpp中实现的<code>train()</code> 和 <code>test()</code>两个函数。搞清楚这两个函数对caffe的调用过程很有帮助。在这之前要提到几个相关的库，虽然与caffe实现关系不大，但完全不懂还是多少有点影响代码的理解。</p>
<h1 id="Google_Code">Google Code</h1><h2 id="glog">glog</h2><p>google出的一个C++轻量级日志库，网络上的教程文档很多<a href="http://google-glog.googlecode.com/svn/trunk/doc/glog.html" target="_blank" rel="external">English docs</a>, <a href="http://www.cnblogs.com/tianyajuanke/archive/2013/02/22/2921850.html" target="_blank" rel="external">Chinese docs</a>, <a href="http://www.cppfans.org/1566.html" target="_blank" rel="external">glog使用与功能修改</a>。<br>看代码的时候充斥这类似<code>CHECK_EQ</code>、 <code>CHECK_NOTNULL</code>、<code>CHECK_STREQ</code>、<code>CHECK_DOUBLE_EQ</code>的函数这就是glog里面的函数，类似ASSERT()的断言。</p>
<h2 id="gflags">gflags</h2><p>它是一个标记选项的库，看了这个<a href="http://download.csdn.net/detail/onlinesoon1/7713855" target="_blank" rel="external">GFLAGS使用手册</a>马上就能明白作用。另外的文档有<a href="http://blog.csdn.net/lezardfu/article/details/23753741" target="_blank" rel="external">Google gflags使用说明</a>。</p>
<h2 id="gtest">gtest</h2><p>Google的开源C++单元测试框架，caffe里面test代码中大量用到，网上教程也是一大堆，中文里面最好的<a href="http://www.cnblogs.com/coderzh/archive/2009/04/06/1426755.html" target="_blank" rel="external">玩转Google开源C++单元测试框架Google Test系列(gtest)(总)</a>，英文文档<a href="https://code.google.com/p/googletest/wiki/Samples" target="_blank" rel="external">googletest</a></p>
<h1 id="关于CPU加速">关于CPU加速</h1><p>Caffe推荐的BLAS（Basic Linear Algebra Subprograms）有三个选择ATLAS，Intel MKL，OpenBLAS。其中ATLAS是caffe是默认选择开源免费，如果没有安装CUDA的不太推荐使用，因为CPU多线程的支持不太好；Intel MKL是商业库要收费，我没有试过但caffe的作者安装的是这个库，估计效果应该是最好的；OpenBLAS开源免费，支持CPU多线程，我安装的就是这个。顺便贴上安装代码，其实很简单：</p>
<h2 id="安装ATLAS">安装ATLAS</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libatlas-base-dev</span><br></pre></td></tr></table></figure>
<h2 id="安装OpenBLAS">安装OpenBLAS</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libopenblas-base</span><br></pre></td></tr></table></figure>
<p>参考链接：<br><a href="https://github.com/BVLC/caffe/issues/16" target="_blank" rel="external">Caffe issues #16: Remove Intel MKL dependency</a><br><a href="https://github.com/BVLC/caffe/issues/79" target="_blank" rel="external">Caffe issues #79: Support multithreading in the CPU mode of Solver::Solve</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/12/14/Caffe学习笔记4-caffe安装需要注意的libraries/" data-id="cibkqhjvo001cx937p3jl2txg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Caffe学习笔记3-Layer的相关学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/12/09/Caffe学习笔记3-Layer的相关学习/" class="article-date">
  <time datetime="2014-12-09T15:18:14.000Z" itemprop="datePublished">2014-12-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LearningCaffe/">LearningCaffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/12/09/Caffe学习笔记3-Layer的相关学习/">Caffe学习笔记3-Layer的相关学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Layer">Layer</h1><p>Layer是所有层的基类，在Layer的基础上衍生出来的有5种Layers：</p>
<ul>
<li>data_layer</li>
<li>neuron_layer</li>
<li>loss_layer</li>
<li>common_layer</li>
<li>vision_layer</li>
</ul>
<p>它们都有对应的[.hpp .cpp]文件声明和实现了各个类的接口。下面一个一个地讲这5个Layer。</p>
<h2 id="data_layer">data_layer</h2><p>先看data_layer.hpp中头文件调用情况：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "boost/scoped_ptr.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "hdf5.h"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "leveldb/db.h"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "lmdb.h"</span></span><br><span class="line"><span class="comment">//前4个都是数据格式有关的文件</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/blob.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/common.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/data_transformer.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/filler.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/internal_thread.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/layer.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/proto/caffe.pb.h"</span></span><br></pre></td></tr></table></figure>
<p>不难看出data_layer主要包含与数据有关的文件。在官方文档中指出data是caffe数据的入口是网络的最低层，并且支持多种格式，在这之中又有5种LayerType：</p>
<ul>
<li><code>DATA</code></li>
<li><code>MEMORY_DATA</code></li>
<li><code>HDF5_DATA</code></li>
<li><code>HDF5_OUTPUT</code></li>
<li>￼<code>￼￼￼￼IMAGE_DATA</code></li>
</ul>
<p>其实还有两种<code>￼￼WINDOW_DATA</code>, <code>DUMMY_DATA</code>用于测试和预留的接口，这里暂时不管。</p>
<h3 id="DATA">DATA</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> BaseDataLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> BasePrefetchingDataLayer : <span class="keyword">public</span> BaseDataLayer&lt;Dtype&gt;, <span class="keyword">public</span> InternalThread</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> DataLayer : <span class="keyword">public</span> BasePrefetchingDataLayer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<p>用于LevelDB或LMDB数据格式的输入的类型，输入参数有<code>source</code>, <code>batch_size</code>, (<code>rand_skip</code>), (<code>backend</code>)。后两个是可选。</p>
<h3 id="MEMORY_DATA">MEMORY_DATA</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> MemoryDataLayer : <span class="keyword">public</span> BaseDataLayer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<p>这种类型可以直接从内存读取数据使用时需要调用<code>MemoryDataLayer::Reset</code>，输入参数有<code>batch_size</code>, <code>channels</code>, <code>height</code>, <code>width</code>。</p>
<h3 id="HDF5_DATA">HDF5_DATA</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> HDF5DataLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<p>HDF5数据格式输入的类型，输入参数有<code>source</code>, <code>batch_size</code>。</p>
<h3 id="HDF5_OUTPUT">HDF5_OUTPUT</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> HDF5OutputLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<p>HDF5数据格式输出的类型，输入参数有<code>file_name</code>。</p>
<h3 id="IMAGE_DATA">IMAGE_DATA</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> ImageDataLayer : <span class="keyword">public</span> BasePrefetchingDataLayer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<p>图像格式数据输入的类型，输入参数有<code>source</code>, <code>batch_size</code>, (<code>rand_skip</code>), (<code>shuffle</code>), (<code>new_height</code>), (<code>new_width</code>)。</p>
<h2 id="neuron_layer">neuron_layer</h2><p>先看neuron_layer.hpp中头文件调用情况</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/blob.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/common.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/layer.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/proto/caffe.pb.h"</span></span><br></pre></td></tr></table></figure>
<p>同样是数据的操作层，neuron_layer实现里大量激活函数，主要是元素级别的操作，具有相同的<code>bottom</code>,<code>top</code>size。<br>Caffe中实现了大量激活函数GPU和CPU的都有很多。它们的父类都是<code>NeuronLayer</code></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> NeuronLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<p>这部分目前没什么需要深究的地方值得注意的是一般的参数设置格式如下（以ReLU为例）：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">￼<span class="tag">layers</span> &#123;</span><br><span class="line">  <span class="attribute">name</span>: <span class="string">"relu1"</span></span><br><span class="line">  ￼<span class="attribute">type</span>: RELU</span><br><span class="line">￼  <span class="attribute">bottom</span>: <span class="string">"conv1"</span></span><br><span class="line">￼  <span class="attribute">top</span>: <span class="string">"conv1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="loss_layer">loss_layer</h2><p>Loss层计算网络误差，loss_layer.hpp头文件调用情况：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/blob.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/common.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/layer.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/neuron_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/proto/caffe.pb.h"</span></span><br></pre></td></tr></table></figure>
<p>可以看见调用了<code>neuron_layers.hpp</code>，估计是需要调用里面的函数计算Loss，一般来说Loss放在最后一层。caffe实现了大量loss function，它们的父类都是<code>LossLayer</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> LossLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<h2 id="common_layer">common_layer</h2><p>先看common_layer.hpp头文件调用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/blob.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/common.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/data_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/layer.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/loss_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/neuron_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/proto/caffe.pb.h"</span></span><br></pre></td></tr></table></figure>
<p>用到了前面提到的<code>data_layers.hpp</code>, <code>loss_layers.hpp</code>, <code>neuron_layers.hpp</code>说明这一层肯定开始有复杂的操作了。<br>这一层主要进行的是<code>vision_layer</code>的连接<br>声明了9个类型的common_layer，部分有GPU实现：</p>
<ul>
<li><code>InnerProductLayer</code></li>
<li><code>SplitLayer</code></li>
<li><code>FlattenLayer</code></li>
<li><code>ConcatLayer</code></li>
<li><code>SilenceLayer</code></li>
<li>(Elementwise Operations) 这里面是我们常说的激活函数层Activation Layers。<ul>
<li><code>EltwiseLayer</code></li>
<li><code>SoftmaxLayer</code></li>
<li><code>ArgMaxLayer</code></li>
<li><code>MVNLayer</code></li>
</ul>
</li>
</ul>
<h3 id="InnerProductLayer">InnerProductLayer</h3><p>常常用来作为全连接层，设置格式为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">￼layers &#123;</span><br><span class="line">  name: <span class="string">"fc8"</span></span><br><span class="line">  ￼￼<span class="built_in">type</span>: INNER_PRODUCT</span><br><span class="line">￼  blobs_lr: <span class="number">1</span>          <span class="comment"># learning rate multiplier for the filters</span></span><br><span class="line">￼  blobs_lr: <span class="number">2</span>          <span class="comment"># learning rate multiplier for the biases</span></span><br><span class="line">￼  weight_decay: <span class="number">1</span>      <span class="comment"># weight decay mu</span></span><br><span class="line">  weight_decay: <span class="number">0</span>      <span class="comment"># weight decay multiplier for the biases</span></span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    ￼num_output: <span class="number">1000</span></span><br><span class="line">￼    weight_filler &#123;</span><br><span class="line">￼      <span class="built_in">type</span>: <span class="string">"gaussian"</span></span><br><span class="line">￼      std: <span class="number">0.01</span></span><br><span class="line">￼    &#125;</span><br><span class="line">￼    bias_filler &#123;</span><br><span class="line">￼      <span class="built_in">type</span>: <span class="string">"constant"</span></span><br><span class="line">￼      value: <span class="number">0</span></span><br><span class="line">￼    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ￼bottom: <span class="string">"fc7"</span></span><br><span class="line">￼  top: <span class="string">"fc8</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="SplitLayer">SplitLayer</h3><p>用于一输入对多输出的场合（对blob）</p>
<h3 id="FlattenLayer">FlattenLayer</h3><p>将n * c * h * w变成向量的格式n * ( c * h * w ) * 1 * 1</p>
<h3 id="ConcatLayer">ConcatLayer</h3><p>用于多输入一输出的场合。</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">￼layers &#123;</span><br><span class="line">  ￼<span class="string">name:</span> <span class="string">"concat"</span></span><br><span class="line">  ￼<span class="string">bottom:</span> <span class="string">"in1"</span></span><br><span class="line">  ￼<span class="string">bottom:</span> <span class="string">"in2"</span></span><br><span class="line">  ￼<span class="string">top:</span> <span class="string">"out"</span></span><br><span class="line">  ￼<span class="string">type:</span> CONCAT</span><br><span class="line">  ￼concat_param &#123;</span><br><span class="line">  ￼  <span class="string">concat_dim:</span> <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="SilenceLayer">SilenceLayer</h3><p>用于一输入对多输出的场合（对layer）</p>
<h3 id="(Elementwise_Operations)">(Elementwise Operations)</h3><p><code>EltwiseLayer</code>, <code>SoftmaxLayer</code>, <code>ArgMaxLayer</code>, <code>MVNLayer</code></p>
<h2 id="vision_layer">vision_layer</h2><p>头文件包含前面所有文件，也就是说包含了最复杂的操作。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/blob.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/common.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/common_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/data_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/layer.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/loss_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/neuron_layers.hpp"</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">include</span> "caffe/proto/caffe.pb.h"</span></span><br></pre></td></tr></table></figure>
<p>它主要是实现Convolution和Pooling操作。主要有以下几个类。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> ConvolutionLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> Im2colLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> LRNLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">class</span> PoolingLayer : <span class="keyword">public</span> Layer&lt;Dtype&gt;</span><br></pre></td></tr></table></figure>
<h3 id="ConvolutionLayer">ConvolutionLayer</h3><p>最常用的卷积操作，设置格式如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">￼layers &#123;</span><br><span class="line">  name: <span class="string">"conv1"</span></span><br><span class="line">￼  <span class="built_in">type</span>: CONVOLUTION</span><br><span class="line">￼  bottom: <span class="string">"data"</span></span><br><span class="line">￼  top: <span class="string">"conv1"</span></span><br><span class="line">￼  blobs_lr: <span class="number">1</span>          <span class="comment"># learning rate multiplier for the filters</span></span><br><span class="line">￼  blobs_lr: <span class="number">2</span>          <span class="comment"># learning rate multiplier for the biases</span></span><br><span class="line">￼  weight_decay: <span class="number">1</span>      <span class="comment"># weight decay multiplier for the filters</span></span><br><span class="line">￼￼  weight_decay: <span class="number">0</span>      <span class="comment"># weight decay multiplier for the biases</span></span><br><span class="line">  convolution_param &#123;</span><br><span class="line">￼    num_output: <span class="number">96</span>     <span class="comment"># learn 96 filters</span></span><br><span class="line">￼    kernel_size: <span class="number">11</span>    <span class="comment"># each filter is 11x11</span></span><br><span class="line">￼    stride: <span class="number">4</span>          <span class="comment"># step 4 pixels between each filter application</span></span><br><span class="line">    ￼weight_filler &#123;</span><br><span class="line">￼￼￼      <span class="built_in">type</span>: <span class="string">"gaussian"</span> <span class="comment"># initialize the filters from a Gaussian</span></span><br><span class="line">      std: <span class="number">0.01</span>        <span class="comment"># distribution with stdev 0.01 (default mean: 0)</span></span><br><span class="line">    &#125;</span><br><span class="line">￼    bias_filler &#123;</span><br><span class="line">￼￼      <span class="built_in">type</span>: <span class="string">"constant"</span> <span class="comment"># initialize the biases to zero (0)</span></span><br><span class="line">￼      value: <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">￼&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Im2colLayer">Im2colLayer</h3><p>与MATLAB里面的im2col类似，即image-to-column transformation，转换后方便卷积计算</p>
<h3 id="LRNLayer">LRNLayer</h3><p>全称local response normalization layer，在Hinton论文中有详细介绍<a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="external">ImageNet Classification with Deep Convolutional Neural Networks
</a>。</p>
<h3 id="PoolingLayer">PoolingLayer</h3><p>即Pooling操作，格式：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layers &#123;</span><br><span class="line">  <span class="property">name</span>: <span class="string">"pool1"</span></span><br><span class="line">  type: POOLING</span><br><span class="line">  bottom: <span class="string">"conv1"</span></span><br><span class="line">  top: <span class="string">"pool1"</span></span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX</span><br><span class="line">    kernel_size: <span class="number">3</span> <span class="comment"># pool over a 3x3 region</span></span><br><span class="line">    stride: <span class="number">2</span>      <span class="comment"># step two pixels (in the bottom blob) between pooling regions</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/12/09/Caffe学习笔记3-Layer的相关学习/" data-id="cibkqhjvq001fx937zb2yf9ni" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Caffe学习笔记2-Caffe的三级结构-Blobs-Layers-Nets" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/12/09/Caffe学习笔记2-Caffe的三级结构-Blobs-Layers-Nets/" class="article-date">
  <time datetime="2014-12-09T11:53:35.000Z" itemprop="datePublished">2014-12-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LearningCaffe/">LearningCaffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/12/09/Caffe学习笔记2-Caffe的三级结构-Blobs-Layers-Nets/">Caffe学习笔记2-Caffe的三级结构(Blobs,Layers,Nets)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>根据Caffe<a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html" target="_blank" rel="external">官方文档</a>介绍，caffe大致可以分为三层结构blob，layer，net。数据的保存，交换以及操作都是以blob的形式进行的，layer是模型和计算的基础，net整和并连接layer。</p>
<h1 id="Blobs">Blobs</h1><p>Blob是Caffe的基本数据结构，具有CPU和GPU之间同步的能力,它是4维的数组(Num, Channels, Height, Width)。<br>设Blob数据维度为 number N x channel K x height H x width W，Blob是row-major保存的，因此在(n, k, h, w)位置的值物理位置为((n * K + k) * H + h) * W + w，其中Number/N是batch size。<br>Blob同时保存了<code>data</code>和<code>diff</code>(梯度)，访问<code>data</code>或<code>diff</code>有两种方法:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> Dtype* cpu_data() <span class="keyword">const</span>; <span class="comment">//不修改值</span></span><br><span class="line">Dtype* mutable_cpu_data();     <span class="comment">//修改值</span></span><br></pre></td></tr></table></figure>
<p>Blob会使用SyncedMem自动决定什么时候去copy data以提高运行效率，通常情况是仅当gnu或cpu修改后有copy操作，文档里面给了一个例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">￼<span class="comment">// Assuming that data are on the CPU initially, and we have a blob.</span></span><br><span class="line"><span class="keyword">const</span> Dtype* foo;</span><br><span class="line">Dtype* bar;</span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// no data copied since both have up-to-date contents.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// no data copied.</span></span><br><span class="line"><span class="comment">// ... some operations ...</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// no data copied when we are still on GPU.</span></span><br><span class="line">foo = blob.cpu_data(); <span class="comment">// data copied gpu-&gt;cpu, since the gpu side has modified the data</span></span><br><span class="line">foo = blob.gpu_data(); <span class="comment">// no data copied since both have up-to-date contents</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// still no data copied.</span></span><br><span class="line">bar = blob.mutable_gpu_data(); <span class="comment">// data copied cpu-&gt;gpu.</span></span><br><span class="line">bar = blob.mutable_cpu_data(); <span class="comment">// data copied gpu-&gt;cpu.</span></span><br></pre></td></tr></table></figure>
<p><em>(顺便查了下一直有疑问<a href="http://baike.baidu.com/link?url=coXh-sAgljTqCl-srJQhwHVkq5i24izowBphWUtQUioR8YOVC_b3tf4-ojGZ5VCCbS9ShH4_XE2_31bw5Ne3KK" target="_blank" rel="external">foo</a>是什么意思。。)</em></p>
<h1 id="Layer">Layer</h1><p>所有的Pooling，Convolve，apply nonlinearities等操作都在这里实现。在Layer中input data用<code>bottom</code>表示output data用<code>top</code>表示。每一层定义了三种操作<code>setup</code>（Layer初始化）, <code>forward</code>（正向传导，根据input计算output）, <code>backward</code>（反向传导计算，根据output计算input的梯度）。<code>forward</code>和<code>backward</code>有GPU和CPU两个版本的实现。</p>
<h1 id="Net">Net</h1><p>Net由一系列的Layer组成(无回路有向图DAG)，Layer之间的连接由一个文本文件描述。模型初始化<code>Net::Init()</code>会产生blob和layer并调用<code>Layer::SetUp</code>。在此过程中<code>Net</code>会报告初始化进程。这里的初始化与设备无关，在初始化之后通过<code>Caffe::set_mode()</code>设置<code>Caffe::mode()</code>来选择运行平台CPU或GPU，结果是相同的。</p>
<hr>
<h1 id="模型的格式">模型的格式</h1><p>模型定义在<code>.prototxt</code>文件中，训练好的模型在<code>model</code>目录下<code>.binaryproto</code>格式的文件中。模型的格式由<code>caffe.proto</code>定义。采用Google Protocol Buffer可以节省空间还有它对C++和Pyhton的支持也很好。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/12/09/Caffe学习笔记2-Caffe的三级结构-Blobs-Layers-Nets/" data-id="cibkqhjvr001ix937xb6rw180" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Caffe学习笔记1-安装以及代码结构" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/12/09/Caffe学习笔记1-安装以及代码结构/" class="article-date">
  <time datetime="2014-12-09T11:51:36.000Z" itemprop="datePublished">2014-12-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/LearningCaffe/">LearningCaffe</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/12/09/Caffe学习笔记1-安装以及代码结构/">Caffe学习笔记1-安装以及代码结构</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="安装">安装</h1><p>按照<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="external">官网教程</a>安装，我在 OS X 10.9 和 Ubuntu 14.04 上面都安装成功了。主要麻烦在于 glog gflags gtest 这几个依赖项是google上面的需要翻墙。由于我用Mac没有CUDA，所以安装时需要设置 CPU_ONLY := 1。</p>
<p><em>如果不是干净的系统，安装还是有点麻烦的比如我在OS X 10.9上面，简直不是一般的麻烦，OS X 10.9 默认的编译器是clang，所以还要修改编译器和重行编译一大堆依赖库。这方面其实网上教程很多，涵盖了各种你可能遇到的问题，多Google下问题还是可以解决的。</em></p>
<h1 id="目录结构">目录结构</h1><p>caffe文件夹下主要文件： <code>这表示文件夹</code></p>
<ul>
<li><code>data</code> 用于存放下载的训练数据</li>
<li><code>docs</code> 帮助文档</li>
<li><code>example</code> 一些代码样例</li>
<li><code>matlab</code> MATLAB接口文件</li>
<li><code>python</code> Python接口文件</li>
<li><code>model</code> 一些配置好的模型参数</li>
<li><code>scripts</code> 一些文档和数据用到的脚本</li>
</ul>
<p>下面是核心代码文件夹：</p>
<ul>
<li><code>tools</code> 保存的源码是用于生成二进制处理程序的，caffe在训练时实际是直接调用这些二进制文件。</li>
<li><code>include</code> Caffe的实现代码的头文件</li>
<li><code>src</code> 实现Caffe的源文件</li>
</ul>
<p><strong>后面的学习主要围绕后面两个文件目录（<code>include</code>和<code>src</code>）下的代码展开</strong></p>
<h1 id="源码结构">源码结构</h1><p>由于<code>include</code>和<code>src</code>两个目录在层次上基本一一对应因此主要分析<code>src</code>即可了解文件结构。</p>
<p><em>这里顺便提到一个有意思的东西，我是在Sublime上面利用SublimeClang插件分析代码的（顺便推荐下这插件，值得花点时间装）。在配置的时候发现会有错误提示找不到”caffe/proto/caffe.pb.h”，去看了下果然没有，但编译的时候没有报错，说明是生成过后又删除了，查看Makefile文件后发现这里用了proto编译的，所以在”src/caffe/proto”下面用CMakeLists文件就可以编译出来了。</em></p>
<ul>
<li><code>src</code><ul>
<li><code>gtest</code> google test一个用于测试的库你make runtest时看见的很多绿色RUN OK就是它，这个与caffe的学习无关，不过是个有用的库</li>
<li><code>caffe</code> 关键的代码都在这里了<ul>
<li><code>test</code> 用gtest测试caffe的代码</li>
<li><code>util</code> 数据转换时用的一些代码。caffe速度快，很大程度得益于内存设计上的优化（blob数据结构采用proto）和对卷积的优化（部分与im2col相关）[1]。</li>
<li><code>proto</code> 即所谓的“Protobuf”[2]，全称“Google Protocol Buffer”，是一种数据存储格式，帮助caffe提速。</li>
<li><code>layers</code> 深度神经网络中的基本结构就是一层层互不相同的网络了，这个文件夹下的源文件以及目前位置“src/caffe”中包含的我还没有提到的所有.cpp文件就是caffe的核心目录下的核心代码了。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="源码主要关系">源码主要关系</h1><p>如上所言我们现在可以知道，caffe核心中的核心是下面的文档和文件:（这部分目前不清楚的地方先参照别人的观点）</p>
<ul>
<li>blob[.cpp .h] 基本的数据结构Blob类[3]。</li>
<li>common[.cpp .h] 定义Caffe类</li>
<li>internal_thread[.cpp .h] 使用boost::thread线程库</li>
<li>net[.cpp .h] 网络结构类Net</li>
<li>solver[.cpp .h] 优化方法类Solver</li>
<li>data_transformer[.cpp .h] 输入数据的基本操作类DataTransformer</li>
<li>syncedmem[.cpp .h] 分配内存和释放内存类CaffeMallocHost，用于同步GPU，CPU数据</li>
<li>layer_factory.cpp layer.h 层类Layer</li>
<li><code>layers</code> 此文件夹下面的代码全部至少继承了类Layer</li>
</ul>
<h1 id="Caffe的官方说明">Caffe的官方说明</h1><p>根据Caffe<a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html" target="_blank" rel="external">官方文档</a>介绍，caffe大致可以分为三层结构blob，layer，net。数据的保存，交换以及操作都是以blob的形式进行的，layer是模型和计算的基础，net整和并连接layer。solver则是模型的优化求解。</p>
<p>[1]: <a href="http://blog.csdn.net/lingerlanlan/article/details/38121443" target="_blank" rel="external">linger: 我所写的CNN框架 VS caffe</a><br>[2]: <a href="http://www.ibm.com/developerworks/cn/linux/l-cn-gpb/" target="_blank" rel="external">Google Protocol Buffer 的使用和原理</a><br>[3]: <a href="http://www.shwley.com/index.php/archives/64/" target="_blank" rel="external">caffe源码简单解析——Blob（1） </a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/12/09/Caffe学习笔记1-安装以及代码结构/" data-id="cibkqhju60000x937navers4b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Caffe/">Caffe</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/23/OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks/" class="article-date">
  <time datetime="2014-11-23T07:41:53.000Z" itemprop="datePublished">2014-11-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Paper/">Paper</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/23/OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks/">OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>CNN Pooling 层的改进，以及用回归预测 object 位置。</p>
<h1 id="Abstract">Abstract</h1><p>本文利用ConvNets整合了classification, localization, detection。利用multi-scale和sliding window与ConvNets结合达到了很好的效果。本文还介绍了一种通过积累predicted bounding boxes来detect和recognize的方法。本文的方法在ILSVRC2013上测试，localization的效果领先，detection和classifications也有相当不错的效果。</p>
<h1 id="Introduction">Introduction</h1><p>ConvNets的优点是训练从头到尾不需要人工干预，缺点是对标签的依赖过强。</p>
<ol>
<li>本文的主要目的是训练出一个可以classify、locate、detect的ConvNets，且所有性能都能提升。</li>
<li>本文还介绍了一种通过积累predicted bounding boxes来detect和recognize的方法。</li>
<li>实验在ImageNet LSVRC 2012和2013上进行，在locate和detect上达到当前最新水平。</li>
<li>ImageNets中的分类数据的size和position差异大，第一种方法用滑窗解决会带来圈偏或多圈的问题，第二种方法是来预测目标的位置和窗的大小，第三种方法是积累每种类别在每个位置和size的可能性。</li>
<li>利用分割的方法实现localization好处是bounding contours不必是矩形，region也无需完美地划定目标。缺点是需要像素级的labels来训练。现在这种方法比较流行，它减少了搜索比例，可以搭配更强大的分类器，但本文的方法却超过了它。</li>
<li>Krizhevsky说过ConvNet可以用在classification、localization但未发表。</li>
</ol>
<h1 id="Vision_Tasks">Vision Tasks</h1><p>本文按照classification, localization, detection顺序一环扣一环，它们共用同一构架，同一套特征。<br>classification, localization都可以有5个可能的结果。bounding box的重合率必须达到50%。<br>每个返回的bounding box必须被正确labeled（即两者不能分离）才认为正确，在同一图像中检测检测的结果数目可以是0～X。</p>
<h1 id="Classification">Classification</h1><p>本文采用的分类器结构与 best ILSVRC12 architecture by Krizhevsky 相似，但许多训练特征未尝试，我们将在第6部分讨论这个问题。</p>
<h2 id="Model_Design_and_Training">Model Design and Training</h2><p>训练模型同Krizhevsky提出的fixed input size approach（A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS 2012: Neural Information Processing Systems. ）但classification变为multi-scale。<br>模型详细结构见图1。Layers 1-5 与 Krizhevsky方法相似用了rectification (“relu”) non-linearities and max pooling，但有以下区别：没有用contrast normalization、pooling region不重叠、1st 2nd layer feature maps更大。<br>图2给出了第1、2层的filter，第1层主要是orientated edges、patterns、斑点，第2层形式多样diffuse、strong line structures、oriented edges。<br><img src="1.png" alt="图1"></p>
<h2 id="Multi-Scale_Classification">Multi-Scale Classification</h2><p>滑窗的方法在ConvNets下十分高效，这部分方法同Krizhevsky，只不过我们探测的是整个图像。<br>pooled和presented to the classifier的过程<br><img src="2.png" alt="图2"></p>
<h2 id="Results">Results</h2><p>实验对比，与期望相同，fewer scales影响性能，单模型效果最糟糕。fine stride technique对性能有微小提升，但它对multi-scale gains非常重要。</p>
<h2 id="ConvNets_and_Sliding_Window">ConvNets and Sliding Window</h2><p>从学习的角度看ConvNets十分高效，原因是它共享多重位置中权重使得filter更加一般化，以及通过积累梯度使学习速度更快。<br>相对于其他结构必须计算整个流程，ConvNets在密集的应用中速度很快，没有多余计算。<br>ConvNets中相邻的输出单元共享低层。（见图3）<br>最后一层完全连接connected linear layers，detection时它直接被1x1的卷机核替代，然后仅仅就是卷积、max-pooling、thresholding operation。<br><img src="3.png" alt="图3"></p>
<h1 id="Localization">Localization</h1><p>把分类器层替换为regression network来训练预测对象bounding boxes，然后结合这些预测核每个位置分类器的结果。</p>
<h2 id="Generating_Predictions">Generating Predictions</h2><p>同时运行classifier和regressor networks，只有最后一个regression layers在classification network计算完后需要再计算。最后可以得到每个bounding boxes的置信区间。</p>
<h2 id="Regressor_Training">Regressor Training</h2><p>结构见图5，它的输入是第5层的pooled feature maps，有2个完全连接size 4096、1024 channels隐层，每个类的输出层不同。有4个unit specify the coordinates for the bounding box edges。<br>前面的multi-scale能提升预测的性能<br><img src="5.png" alt="图5"></p>
<h2 id="Combining_Predictions">Combining Predictions</h2><p>利用图7的greedy merge strategy预测位置，算法如下：<br>match score是两个bounding boxes中心的距离和它们交叉面积之和。<br>box merge计算两bounding boxes的coordinates的平均值。<br>最后的预测是找maximum class scores的那个merged bounding boxes。<br>图4中给了一个合并的例子，3.2中的方法提高预测效果。最后bounding合并并积累成少量的对象。<br><img src="4.png" alt="图4"></p>
<h1 id="Detection">Detection</h1><p>除了spatial manner以外与分类相同，多重位置被同时训练，不同在于localization，它还是预测了位置，boot- strapping passes。we perform negative training on the fly？选每幅图中随机的一个负样本。</p>
<h1 id="Discussion">Discussion</h1><p>overfeat ranks 4th on classification, 1st on localization and 1st on detection.<br>本文解释了ConvNets在localization and detection上高效的原因。<br>提出了一个结合分类、定位、检测的流程，它共享特征直接从像素点学习。它用multi-scale、sliding window方法。<br>overfeat可以在以下方向改进：</p>
<ol>
<li>localization时没有实时反馈到整个网络，反馈后效果会更好。</li>
<li>本文使用 l2 loss（衰退网络的参数），而不是直接优化intersection-over-union (IOU) 准则。</li>
<li>变换bounding box的参数帮助输出去相关，这有助于网络训练</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/11/23/OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks/" data-id="cibkqhjvd000tx9378ydjdahh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/">CNN</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Book/">Book</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Code/">Code</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LearningCaffe/">LearningCaffe</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Note/">Note</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BLAS/">BLAS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Boost/">Boost</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Caffe/">Caffe</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Thread/">Thread</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Torch/">Torch</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/《Deep-Learning》/">《Deep Learning》</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/BLAS/" style="font-size: 10px;">BLAS</a> <a href="/tags/Boost/" style="font-size: 10px;">Boost</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Caffe/" style="font-size: 20px;">Caffe</a> <a href="/tags/Thread/" style="font-size: 10px;">Thread</a> <a href="/tags/Torch/" style="font-size: 13.33px;">Torch</a> <a href="/tags/《Deep-Learning》/" style="font-size: 16.67px;">《Deep Learning》</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/02/26/近期DL重要论文整理长期更新/">近期DL重要论文整理长期更新</a>
          </li>
        
          <li>
            <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记3-CNN/">《Deep Learning》(Bengio)读书笔记3-CNN</a>
          </li>
        
          <li>
            <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记2-ML基本知识/">《Deep Learning》(Bengio)读书笔记2-ML基本知识</a>
          </li>
        
          <li>
            <a href="/2015/02/20/《Deep-Learning》-Bengio-读书笔记1-DL简介/">《Deep Learning》(Bengio)读书笔记1-DL简介</a>
          </li>
        
          <li>
            <a href="/2015/01/02/Caffe学习笔记5-BLAS与boost-thread加速/">Caffe学习笔记5-BLAS与boost::thread加速</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 YuFeiGan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>